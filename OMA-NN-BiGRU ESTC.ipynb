{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15d2e5f8",
   "metadata": {},
   "source": [
    "## 1. Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e347d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import Library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import random as r\n",
    "import copy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Use Tensorflow-Keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, concatenate, Flatten, Dropout\n",
    "from keras.layers import LSTM, Bidirectional, GRU\n",
    "from keras.utils import plot_model\n",
    "import tensorflow.keras.backend as K\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#Use Sklearn for evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, mean_absolute_error, r2_score\n",
    "from scipy.stats import pearsonr \n",
    "import scipy.stats as stats\n",
    "from scipy.special import expit\n",
    "\n",
    "#Add\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01d7ca0",
   "metadata": {},
   "source": [
    "## 2. Data Loader and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2386739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "raw_csv_data=pd.read_csv(r\"ESTC Denorm.csv\")\n",
    "df=raw_csv_data.copy()\n",
    "\n",
    "# Normalize\n",
    "x = df.values #returns a numpy array\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# x_scaled = min_max_scaler.fit_transform(x)\n",
    "df = pd.DataFrame(x)\n",
    "\n",
    "# For training + validation 60% + 20% = 80% or 0.8\n",
    "split = df.shape[0]*0.8\n",
    "# the training and validation will be splited later in the model compiler\n",
    "\n",
    "train_dataset = df.loc[:split, :]\n",
    "test_dataset = df.loc[split:, :]\n",
    "print(train_dataset.shape, test_dataset.shape)\n",
    "\n",
    "# Split Time-independent Variables Input for NN\n",
    "train_xi = train_dataset.loc[:,1:14]\n",
    "test_xi = test_dataset.loc[:,1:14]\n",
    "\n",
    "print(train_xi.shape)\n",
    "print(test_xi.shape)\n",
    "\n",
    "# Split Time-dependent Variables Input for BiGRU\n",
    "train_xg = train_dataset.loc[:,15:]\n",
    "test_xg = test_dataset.loc[:,15:]\n",
    "\n",
    "print(train_xg.shape)\n",
    "print(test_xg.shape)\n",
    "\n",
    "# Reshape input data dimension from 2D to 3D for BiGRU input\n",
    "train_xd = train_xg.values.reshape(train_xg.shape[0], 3, 5) # (batch size, timesteps, features)\n",
    "test_xd = test_xg.values.reshape(test_xg.shape[0], 3, 5) # (batch size, timesteps, features)\n",
    "print(train_xd.shape, test_xd.shape)\n",
    "\n",
    "# Output or Target Variable\n",
    "train_y = train_dataset.iloc[:,0]\n",
    "test_y = test_dataset.iloc[:,0]\n",
    "print(train_y.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47921d71",
   "metadata": {},
   "source": [
    "## 3. OMA-NN-BiGRU Model\n",
    "\n",
    "Author : Frederik Elly Gosal\n",
    "    \n",
    "    Always make the input for the model as follows:\n",
    "\n",
    "    For NN (Time-Independent) (none, features):\n",
    "    Training variables: train_xi.\n",
    "        Test variables: test_xi.\n",
    "    \n",
    "    For GRU (Time-Dependent) (none, timesteps, features):\n",
    "    Training variables: train_xd.\n",
    "        Test variables: test_xd.\n",
    "    \n",
    "    For output :\n",
    "    Training variables: train_y.\n",
    "        Test variables: test_y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8061232",
   "metadata": {},
   "source": [
    "### 3.1 Optimize the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1be2133",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def objfun(x):\n",
    "    nn_neurons = int(x[0])\n",
    "    bigru_units = int(x[1])\n",
    "    nn_dense_neurons = int(x[2])\n",
    "    bigru_dense_neurons = int(x[3])\n",
    "    nn_dropout_rate = x[4]\n",
    "    bigru_dropout_rate = x[5]\n",
    "    lr = x[6]\n",
    "    \n",
    "    # NN-BiGRU Model\n",
    "    \n",
    "    # BiGRU\n",
    "    gru_input = Input(shape=(train_xd.shape[1],train_xd.shape[2]) , name='Left_input')\n",
    "    gru_layer = Bidirectional(GRU(bigru_units, activation='tanh', return_sequences=True))(gru_input)\n",
    "    gru_layer = Dropout(bigru_dropout_rate)(gru_layer)\n",
    "    gru_layer = Bidirectional(GRU(bigru_units, activation='tanh', return_sequences=False))(gru_layer)\n",
    "    gru_layer = Dropout(bigru_dropout_rate)(gru_layer)\n",
    "    gru_dense = Dense(bigru_dense_neurons, activation='linear')(gru_layer)\n",
    "\n",
    "    # NN\n",
    "    dnn_input = Input(shape=(train_xi.shape[1]), name='Right_input')\n",
    "    dnn_layer = Dense(nn_neurons, activation='relu')(dnn_input)\n",
    "    dnn_layer = Dropout(nn_dropout_rate)(dnn_layer)\n",
    "    dnn_layer = Dense(nn_neurons, activation='relu')(dnn_layer)\n",
    "    dnn_layer = Dropout(nn_dropout_rate)(dnn_layer)\n",
    "    dnn_dense = Dense(nn_dense_neurons, activation='linear')(dnn_layer)\n",
    "\n",
    "    # Combine the ouput using Concatenation (concat layer)\n",
    "    concat = concatenate([gru_dense, dnn_dense], name='Concatenate')\n",
    "    final_model_output = Dense(1, activation='sigmoid')(concat)\n",
    "    final_model = Model(inputs=[gru_input, dnn_input], outputs=final_model_output,\n",
    "                        name='Final_output')\n",
    "    \n",
    "    # Train the model and retrun val_loss as Objective function\n",
    "    final_model.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='mse')\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "    history = final_model.fit([train_xd, train_xi], train_y, epochs=1000, batch_size=train_y.shape[0], \n",
    "                          validation_data=([test_xd, test_xi], test_y), shuffle=False, verbose=0, callbacks=[callback])\n",
    "    # Validation_split=0.25, which means it takes 25% of the training data, in other word 25% from 80% equals 20% from 100%\n",
    "    \n",
    "    K.clear_session()\n",
    "    fit = np.min(history.history['val_loss'])\n",
    "    return fit\n",
    "\n",
    "def rangeCheck(x):\n",
    "    for i in range(0,x.size):\n",
    "        if x[i]<Lb[i]:\n",
    "            x[i]=Lb[i]\n",
    "        if x[i]>Ub[i]:\n",
    "            x[i]=Ub[i]\n",
    "                        \n",
    "nVar=7 # number of variable\n",
    "Ub=np.asarray([200, 100, 20, 20, 0.5, 0.5, 0.1]) # (nn_neurons, gru_units, dense_neurons, dropout_rate, lr)\n",
    "Lb=np.asarray([5, 5, 1, 1, 0, 0, 0.0001])\n",
    "maxiter=50 # maximum iteration\n",
    "npop=10 # number of population\n",
    "\n",
    "# Credit:\n",
    "# The Optical Microscope Algorithm: A Novel Metaheuristic Inspired by Microscope Magnification\n",
    "# Author and programmer:                                                  \n",
    "#          Professor        Min-Yuan Cheng                                   \n",
    "#          Ph.D. Student    Moh Nur Sholeh                                   \n",
    "#   Written by Moh Nur Sholeh                                                \n",
    "#   Computer Integrated Construction (CIC) Lab                               \n",
    "#   National Taiwan University of Science and Technology, Taipei, Taiwan \n",
    "\n",
    "#1 Naked eye phase\n",
    "x=np.zeros((npop,nVar))\n",
    "fit=np.zeros((npop,1))\n",
    "x[0,]=np.random.rand(1,nVar)\n",
    "for i in range(1,npop):\n",
    "    x[i,]=x[i-1,]*(1-x[i-1,])\n",
    "A = []     \n",
    "for i in tqdm(range (npop),desc=\"Loading...\"):\n",
    "    x[i,]=Lb+x[i,]*(Ub-Lb)\n",
    "    fit[i,]=objfun(x[i,])\n",
    "\n",
    "#OMA Main Loop\n",
    "for it in range(0, maxiter):\n",
    "    idx=np.argmin(fit)\n",
    "    bestsol=x[idx,]\n",
    "    bestmag=fit[idx,]\n",
    "#     print(\"Iter {}: {}\".format(it,bestmag))\n",
    "    for i in tqdm(range (npop),desc=\"Loading...\"):\n",
    "        #2 Objective Lens Phase\n",
    "        xnew=bestsol+np.random.rand(nVar)*1.4*(x[i,])\n",
    "        rangeCheck(xnew)\n",
    "        fitnew=objfun(xnew)\n",
    "        if fitnew<fit[i,]:\n",
    "            fit[i,]=fitnew\n",
    "            x[i,]=xnew\n",
    "        \n",
    "        #3 Eyepiece phase\n",
    "        j=r.randrange(0,npop)\n",
    "        while j==i:\n",
    "            j=r.randrange(0,npop)\n",
    "        if fit[i,]>=fit[j,]:\n",
    "            space=x[j,]-x[i,]\n",
    "        else :\n",
    "            space=x[i,]-x[j,]\n",
    "        xnew=x[i,]+np.random.rand(nVar)*(0.55*space)\n",
    "        rangeCheck(xnew)\n",
    "        fitnew=objfun(xnew)\n",
    "        if fitnew<fit[i,]:\n",
    "            fit[i,]=fitnew\n",
    "            x[i,]=xnew\n",
    "    A.append(bestmag[0]) # record the best fitness\n",
    "    \n",
    "idx=np.argmin(fit)\n",
    "bestsol=x[idx,]\n",
    "bestmag=fit[idx,]\n",
    "# Plot the objective value for each generation\n",
    "fig = plt.figure()\n",
    "plt.plot(A)\n",
    "fig.suptitle('OMA')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Objective function value')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc64ba1",
   "metadata": {},
   "source": [
    "#### 3.1.1 Print the optimal Architecture Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fac96ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MSE = {:.6f}\" .format(bestmag[0]))\n",
    "\n",
    "print(' ')\n",
    "\n",
    "print(\"Optimal Parameters:\")\n",
    "print(\"NN Number of Neurons           = {:.0f}\" .format(int(bestsol[0])))\n",
    "print(\"BiGRU Number of Units          = {:.0f}\" .format(int(bestsol[1])))\n",
    "print(\"NN Dense Number of Neurons     = {:.0f}\" .format(int(bestsol[2])))\n",
    "print(\"BiGRU Dense Number of Neurons  = {:.0f}\" .format(int(bestsol[3])))\n",
    "print(\"NN Dropout Rate                = {:.4f}\" .format(bestsol[4]))\n",
    "print(\"BiGRU Dropout Rate             = {:.4f}\" .format(bestsol[5]))\n",
    "print(\"Learning Rate                  = {:.4f}\" .format(bestsol[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc6674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the actual optimal dropout rate and learning rate value used in Optimized Inference Model\n",
    "print(\"NN Dropout Rate     = \", bestsol[4])\n",
    "print(\"BiGRU Dropout Rate  = \", bestsol[5])\n",
    "print(\"Learning Rate       = \", bestsol[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac683e4",
   "metadata": {},
   "source": [
    "#### 3.1.2 Train the model with Optimal Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e81043",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nn_neurons = int(bestsol[0])\n",
    "bigru_units = int(bestsol[1])\n",
    "nn_dense_neurons = int(bestsol[2])\n",
    "bigru_dense_neurons = int(bestsol[3])\n",
    "nn_dropout_rate = bestsol[4]\n",
    "bigru_dropout_rate = bestsol[5]\n",
    "lr = bestsol[6]\n",
    "\n",
    "# NN-BiGRU Model\n",
    "    \n",
    "# BiGRU\n",
    "gru_input = Input(shape=(train_xd.shape[1],train_xd.shape[2]) , name='Left_input')\n",
    "gru_layer = Bidirectional(GRU(bigru_units, activation='tanh', return_sequences=True))(gru_input)\n",
    "gru_layer = Dropout(bigru_dropout_rate)(gru_layer)\n",
    "gru_layer = Bidirectional(GRU(bigru_units, activation='tanh', return_sequences=False))(gru_layer)\n",
    "gru_layer = Dropout(bigru_dropout_rate)(gru_layer)\n",
    "gru_dense = Dense(bigru_dense_neurons, activation='linear')(gru_layer)\n",
    "\n",
    "# NN\n",
    "dnn_input = Input(shape=(train_xi.shape[1]), name='Right_input')\n",
    "dnn_layer = Dense(nn_neurons, activation='relu')(dnn_input)\n",
    "dnn_layer = Dropout(nn_dropout_rate)(dnn_layer)\n",
    "dnn_layer = Dense(nn_neurons, activation='relu')(dnn_layer)\n",
    "dnn_layer = Dropout(nn_dropout_rate)(dnn_layer)\n",
    "dnn_dense = Dense(nn_dense_neurons, activation='linear')(dnn_layer)\n",
    "\n",
    "# Combine the ouput using Concatenation (concat layer)\n",
    "concat = concatenate([gru_dense, dnn_dense], name='Concatenate')\n",
    "final_model_output = Dense(1, activation='sigmoid')(concat)\n",
    "final_model = Model(inputs=[gru_input, dnn_input], outputs=final_model_output,\n",
    "                        name='Final_output')\n",
    "    \n",
    "\n",
    "final_model.compile(optimizer=tf.keras.optimizers.Adam(lr), loss='mse')\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='best_weights.hdf5', \n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             monitor='val_loss', \n",
    "                             mode='min', \n",
    "                             verbose=1)\n",
    "\n",
    "history = final_model.fit([train_xd, train_xi], train_y, epochs=500, batch_size=train_y.shape[0], \n",
    "                          validation_data=([test_xd, test_xi], test_y),callbacks=[checkpoint], shuffle=False)\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='val')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7f5cbd",
   "metadata": {},
   "source": [
    "### 3.2 Optimize the Output Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc2c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best weights to the model\n",
    "final_model.load_weights('best_weights.hdf5')\n",
    "y_pred = final_model.predict([test_xd, test_xi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeecb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the neuron output in concat layer\n",
    "concat_model = Model(inputs=final_model.input, outputs=final_model.get_layer('Concatenate').output)\n",
    "concatenated_output = concat_model.predict([train_xd, train_xi])\n",
    "print(concatenated_output.shape) # (number of train-data, num of neuron)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b952d7bf",
   "metadata": {},
   "source": [
    "## NOTE*\n",
    "If the output is not satisfying, it is due to the output weights are stuck at the local best solution.\n",
    "\n",
    "Therefore, to get better result you need to run the OMA again from this section by click this section box, click cell on the menu bar, then click run all below.\n",
    "\n",
    "Do the above mentioned steps until you are satisfied with the result :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881d8a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = concatenated_output\n",
    "targets = train_y.values.reshape(-1)\n",
    "\n",
    "def fitness(weights, data, targets):\n",
    "    predictions = expit(np.dot(data, weights)) # sigmoid function applied\n",
    "    error = np.mean((targets - predictions) ** 2.0) # MSE\n",
    "    return error\n",
    "\n",
    "def objfun1(weights):\n",
    "    return fitness(weights, data, targets)\n",
    "\n",
    "def rangeCheck(x):\n",
    "    for i in range(0,x.size):\n",
    "        if x[i]<Lb1:\n",
    "            x[i]=Lb1\n",
    "        if x[i]>Ub1:\n",
    "            x[i]=Ub1\n",
    "            \n",
    "nVar1=concatenated_output.shape[1]\n",
    "Ub1=1\n",
    "Lb1=-1\n",
    "maxiter1=100\n",
    "npop1=100\n",
    "\n",
    "# Credit:\n",
    "# The Optical Microscope Algorithm: A Novel Metaheuristic Inspired by Microscope Magnification\n",
    "# Author and programmer:                                                  \n",
    "#          Professor        Min-Yuan Cheng                                   \n",
    "#          Ph.D. Student    Moh Nur Sholeh                                   \n",
    "#   Written by Moh Nur Sholeh                                                \n",
    "#   Computer Integrated Construction (CIC) Lab                               \n",
    "#   National Taiwan University of Science and Technology, Taipei, Taiwan\n",
    "\n",
    "#1 Naked eye phase\n",
    "x=np.zeros((npop1,nVar1))\n",
    "fit=np.zeros((npop1,1))\n",
    "x[0,]=np.random.rand(1,nVar1)\n",
    "for i in range(1,npop1):\n",
    "    x[i,]=x[i-1,]*(1-x[i-1,])\n",
    "    \n",
    "for i in range(0,npop1):\n",
    "    x[i,]=Lb1+x[i,]*(Ub1-Lb1)\n",
    "    fit[i,]=objfun1(x[i,])\n",
    "\n",
    "#OMA Main Loop\n",
    "for it in range(0, maxiter1):\n",
    "    idx=np.argmin(fit)\n",
    "    bestsol1=x[idx,]\n",
    "    bestmag1=fit[idx,]\n",
    "#     print(\"Iter {}: {}\".format(it,bestmag1))\n",
    "    for i in range(0,npop1):\n",
    "        #2 Objective Lens Phase\n",
    "        xnew=bestsol1+np.random.rand(nVar1)*1.4*(x[i,])\n",
    "        rangeCheck(xnew)\n",
    "        fitnew=objfun1(xnew)\n",
    "        if fitnew<fit[i,]:\n",
    "            fit[i,]=fitnew\n",
    "            x[i,]=xnew\n",
    "        \n",
    "        #3 Eyepiece phase\n",
    "        j=r.randrange(0,npop1)\n",
    "        while j==i:\n",
    "            j=r.randrange(0,npop1)\n",
    "        if fit[i,]>=fit[j,]:\n",
    "            space=x[j,]-x[i,]\n",
    "        else :\n",
    "            space=x[i,]-x[j,]\n",
    "        xnew=x[i,]+np.random.rand(nVar1)*(0.55*space)\n",
    "        rangeCheck(xnew)\n",
    "        fitnew=objfun1(xnew)\n",
    "        if fitnew<fit[i,]:\n",
    "            fit[i,]=fitnew\n",
    "            x[i,]=xnew\n",
    "            \n",
    "idx=np.argmin(fit)\n",
    "bestsol1=x[idx,]\n",
    "bestmag1=fit[idx,]\n",
    "print(bestsol1)\n",
    "print(\"Train_MSE:\", bestmag1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672c4834",
   "metadata": {},
   "source": [
    "#### 3.2.1 Print the optimal output weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5fe0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, weight in enumerate(bestsol1):\n",
    "    print(\"W{} = {:.6f}\".format(i+1, weight))\n",
    "    \n",
    "sum_matrix = np.sum(bestsol1)\n",
    "print(\"The sum of the weight:\", sum_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59839c1",
   "metadata": {},
   "source": [
    "### 3.3 Make Prediction Using Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f17e605",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict using the testing dataset\n",
    "y_out_concat = concat_model.predict([test_xd, test_xi])\n",
    "y_pred_OMA = expit(np.dot(y_out_concat, bestsol1))\n",
    "print(y_pred_OMA.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60aa25e",
   "metadata": {},
   "source": [
    "## 4. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51ccd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denormalize Output\n",
    "max_y = 1 # target variable's maximum value from whole dataset\n",
    "min_y = 0 # target variable's minimum value from whole dataset\n",
    "\n",
    "test_y_act = test_y * (max_y - min_y) + min_y\n",
    "test_y_act = test_y_act.values.reshape(-1)\n",
    "y_pred_OMA_act = y_pred_OMA * (max_y - min_y) + min_y\n",
    "y_pred_OMA_act = y_pred_OMA_act.reshape(-1)\n",
    "y_pred_act = y_pred * (max_y - min_y) + min_y\n",
    "y_pred_act = y_pred_act.reshape(-1)\n",
    "\n",
    "# act means actual value, the denormalized value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b647001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Metrics\n",
    "corr, _ = pearsonr(test_y_act, y_pred_OMA_act)\n",
    "r2 = r2_score(test_y_act, y_pred_OMA_act)\n",
    "mape = mean_absolute_percentage_error(test_y_act, y_pred_OMA_act)\n",
    "mae = mean_absolute_error(test_y, y_pred_OMA)\n",
    "mse = mean_squared_error(test_y, y_pred_OMA, squared = True)\n",
    "rmse = mean_squared_error(test_y, y_pred_OMA, squared = False)\n",
    "\n",
    "# mae, mse, rmse are using normalize value, not actual. Therefore, the value will be in range of 0 and 1 for model comparison\n",
    "\n",
    "print(\"R: %.4f\" %corr)\n",
    "print(\"R2: %.4f\" %r2)\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print('RMSE: %.4f' % rmse)\n",
    "print('MAE: %.4f' % mae)\n",
    "print('MAPE: %.4f' % mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6944bd8",
   "metadata": {},
   "source": [
    "##### Result with ADAM weight optimizer for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619f20b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation Metrics\n",
    "corr, _ = pearsonr(test_y_act, y_pred_act)\n",
    "r2 = r2_score(test_y_act, y_pred_act)\n",
    "mape = mean_absolute_percentage_error(test_y_act, y_pred_act)\n",
    "mae = mean_absolute_error(test_y, y_pred)\n",
    "mse = mean_squared_error(test_y, y_pred, squared = True)\n",
    "rmse = mean_squared_error(test_y, y_pred, squared = False)\n",
    "\n",
    "print(\"R: %.4f\" %corr)\n",
    "print(\"R2: %.4f\" %r2)\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print('RMSE: %.4f' % rmse)\n",
    "print('MAE: %.4f' % mae)\n",
    "print('MAPE: %.4f' % mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ffdeb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
