1. NN(Neural Network)
인간 두뇌의 복잡한 구조와 기능에서 영감을 얻은 계산모델
신경망은 서로 연결된 노드 또는 뉴런의 계층으로 구성된다. 일반적으로 원시데이터를 수신하는 입력 계층, 
계산을 통해서 데이터를 처리하는 하나 이상의 은닉층, 분류 라벨이나 예측값과 같은 최종 결과를 생성하는 출력 계층으로 이루어져 있다. 
뉴런 사이이 각 연결에는 네트워크 계산 시 연결의 중요성을 나타내는 '가중치'가 있는데, 학습과정에서 계속 업데이트 되며
이 가중치가 모델의 예측 능력을 결정한다. 
뉴런은 ReLU, Sigmoid, Tanh와 같은 활성화 함수를 사용하여 입력을 처리하며, 이 함수는 다음 계층으로 전달할 출력 신호를 결정한다.
네트워크는 훈련과정에서 이러한 가중치를 조정, 학습한다. 

딥러닝을 사용하는 모델은 기본적으로 여러 개의 숨겨진 레이어를 가진다. 여러 개의 레이어 덕분에 대규모 데이터 세트에서 복잡한
패턴과 계층적 특징을 학습할 수 있다. 


* ReLU(Rectified Linear Unit)는 인공신경망에서 가장 널리 쓰이는 활성화 함수(activation function) 중 하나이다. 
ReLU(x)=max(0,x)



2. transformer모델
transformer모델은 2017년 구글의 연구팀이 발표한 자연어 처리(NLP, Natural Language Processing) 모델이다.
기존의 RNN(Recurrent Neural Network)과 CNN(Convolutional Neural Network) 기반 모델이 가진 단점을 극복하고, 
병렬 처리 성능과 성능 향상을 동시에 달성한 모델이다. 

트랜스포머는 기본적으로 인코더(Encoder)와 디코더(Decoder) 구조로 이루어져 있는데
인코더는 입력 문장을 받아 의미를 함축한 표현(Embedding)을 생성하고, 디코더는 인코더의 출력 정보를 바탕으로 새로운 문장을 생성한다.
기존의 RNN이나 LSTM은 순차적으로 데이터를 처리하기 때문에 병렬 처리가 어렵고 긴 문장을 다룰 때 성능이 저하되지만, 
Transformer는 모든 입력을 동시에 처리하면서 입력들 간의 의미관계를 파악하는 데 뛰어난 self-attention 메커니즘을 사용한다. 
Self-Attention은 문장의 모든 단어를 동시에 비교하여, 문장의 각 단어가 서로 얼마나 중요한지를 학습한다.


2.1 트랜스모머 내부 동작 방식
1) Tokenization & Embedding
컴퓨터는 글자를 이해하지 못하기 때문에, 먼저 텍스트 데이터를 숫자로 변환해야 한다. 
Tokenization은 문장을 단어 단위로 나눈다. 
이후 Embedding을 통해 각 단어를 고유한 숫자벡터로 변환하는데, 이 과정에서 단어의 의미를 반영하는 벡터가 생성된다.

2) Positional encoding
Transformer는 RNN처럼 각 단어의 위치 정보를 가지지 않기 때문에 문장 내에서 단어의 위치 정보를 추가하는 과정이 필요하다,
이를 위해 각 단어의 임베팅 벡터에 위치 정보를 더해 모델의 입력으로 사용하는데, 이를 포지셔널 인코딩이라고 한다. 
입력으로 사용되는 임베딩 벡터들이 트랜스포머 입력으로 사용되기 전 포지셔널 인코딩 값이 더해진다.
다음 그림은 [  ] 임베딩 벡터가 인코더 입력으로 사용되기 전 포지셔널 인코딩값이 더해지는 과정을 시각화한 것이다. 



3) Self-Attention 연산






