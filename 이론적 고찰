1. NN(Neural Network)
인간 두뇌의 복잡한 구조와 기능에서 영감을 얻은 계산모델
신경망은 서로 연결된 노드 또는 뉴런의 계층으로 구성된다. 일반적으로 원시데이터를 수신하는 입력 계층, 
계산을 통해서 데이터를 처리하는 하나 이상의 은닉층, 분류 라벨이나 예측값과 같은 최종 결과를 생성하는 출력 계층으로 이루어져 있다. 
뉴런 사이이 각 연결에는 네트워크 계산 시 연결의 중요성을 나타내는 '가중치'가 있는데, 학습과정에서 계속 업데이트 되며
이 가중치가 모델의 예측 능력을 결정한다. 
뉴런은 ReLU, Sigmoid, Tanh와 같은 활성화 함수를 사용하여 입력을 처리하며, 이 함수는 다음 계층으로 전달할 출력 신호를 결정한다.
네트워크는 훈련과정에서 이러한 가중치를 조정, 학습한다. 

딥러닝을 사용하는 모델은 기본적으로 여러 개의 숨겨진 레이어를 가진다. 여러 개의 레이어 덕분에 대규모 데이터 세트에서 복잡한
패턴과 계층적 특징을 학습할 수 있다. 

또한 NN는 시간독립적인 데이터를 처리하는 데 유용하다. 
시간독립적인 데이터란 입력 특성들이 서로 독립적이며, 시간적인 흐름이나 순서가 중요하지 않은 데이터를 말한다.
예를 들면 건축 프로젝트의 특성(공법, 면적, 발주 방식, 건물의 크기)이나 인력 구성, 장비 정보(투입 장비 수, 장비 유형), 자재 단가 등이 있다. 
NN는 입력 벡터의 순서보다는 각 특성의 의미와 값에 집중하고, 입력 순서가 학습에 영향을 주지 않기 때문에 시간독립적인
데이터 처리에 유용하다. 

* ReLU(Rectified Linear Unit)는 인공신경망에서 가장 널리 쓰이는 활성화 함수(activation function) 중 하나이다. 
ReLU(x)=max(0,x)



2. transformer모델
transformer모델은 2017년 구글의 연구팀이 발표한 자연어 처리(NLP, Natural Language Processing) 모델이다.
기존의 RNN(Recurrent Neural Network)과 CNN(Convolutional Neural Network) 기반 모델이 가진 단점을 극복하고, 
병렬 처리 성능과 성능 향상을 동시에 달성한 모델이다. 

트랜스포머는 기본적으로 인코더(Encoder)와 디코더(Decoder) 구조로 이루어져 있는데
인코더는 입력 문장을 받아 의미를 함축한 표현(Embedding)을 생성하고, 디코더는 인코더의 출력 정보를 바탕으로 새로운 문장을 생성한다.
기존의 RNN이나 LSTM은 순차적으로 데이터를 처리하기 때문에 병렬 처리가 어렵고 긴 문장을 다룰 때 성능이 저하되지만, 
Transformer는 모든 입력을 동시에 처리하면서 입력들 간의 의미관계를 파악하는 데 뛰어난 self-attention 메커니즘을 사용한다. 
Self-Attention은 문장의 모든 단어를 동시에 비교하여, 문장의 각 단어가 서로 얼마나 중요한지를 학습한다.
또한 트랜스포머는 포지셔널 인코딩(Positional Encoding)이라는 기법을 사용해 단어의 위치를 순서로 바꾸어 모델이 문장 내 단어의 
순서르 학습할 수 있도록 도와준다. 


(트랜스포머는 하이퍼파라미터인 num_layers개수의 인코더 층을 쌓는다. 인코더를 하나의 층이란 개념으로 생각하면 하나의 인코더 층은 
크게 2개의 서브층(sublayer)으로 나뉘는데, 이는 self-attention과 Feed-Forward-Network 층이다.)

2.1 트랜스모머 내부 동작 방식
1) Tokenization & Embedding
컴퓨터는 글자를 이해하지 못하기 때문에, 먼저 텍스트 데이터를 숫자로 변환해야 한다. 
Tokenization은 문장을 단어 단위로 나눈다. 
이후 Embedding을 통해 각 단어를 고유한 숫자벡터로 변환하는데, 이 과정에서 단어의 의미를 반영하는 벡터가 생성된다.

2) Positional encoding
Transformer는 RNN처럼 각 단어의 위치 정보를 가지지 않기 때문에 문장 내에서 단어의 위치 정보를 추가하는 과정이 필요하다,
이를 위해 각 단어의 임베팅 벡터에 위치 정보를 더해 모델의 입력으로 사용하는데, 이를 포지셔널 인코딩이라고 한다. 
입력으로 사용되는 임베딩 벡터들이 트랜스포머 입력으로 사용되기 전 포지셔널 인코딩 값이 더해진다.
다음 그림은 [  ] 임베딩 벡터가 인코더 입력으로 사용되기 전 포지셔널 인코딩값이 더해지는 과정을 시각화한 것이다. 

트랜스포머는 위치 정보를 가진 값을 만들기 위해 아래 2개의 함수를 사용한다. 
[ sin cos 함수]
임베딩 벡터가 인코더 입력으로 사용되기 전 포지셔널 인코딩 값이 더해지는 과정은 임베딩 벡터가 모여 만들어진 문장 벡터
행렬과 포지셔널 인코딩 행렬의 덧셈연산을 통해 이루어진다.
pos는 입력 문장에서의 임베딩 벡터 위치를 나타내며, i는 임베딩 벡터 내 차원의 인덱스를 의미한다. 
임베딩 벡터 내 각 차원의 인덱스가 짝수인 경우(pos, 2i) 사인 함수 값을 이용하고 홀수인 경우(pos, 2i+1) 코사인 함수 값을 사용한다.
dmodel은 트랜스포머의 모든 층의 출력 차원을 의미하는 하이퍼파라미터이다. 
이렇게 포지셔널 인코딩 방법을 사용하면 순서 정보가 보존된다.


3) Self-Attention 연산
Self-Attetion은 트랜스포머의 핵심인데, 문장의 모든 단어들이 서로 얼마나 관련이 있는지 학습하는 과정이다. 

4) Multi-Head Attention 기법
셀프 어텐션을 한번만 수행하면 특정 패턴만 학습할 수 있기 때문에, 여러 개의 어텐션을 병렬로 실행하는 멀티 헤드 어텐션 기법을 사용한다. 

5) Feed Forward Network, FFN
어텐션을 거친 벡터는 추가적인 신경망(FNN)을 거쳐 더욱 정교하게 변환된다.
이 신경망은 입력 데이터를 비선형적으로 변환하여 더 의미있는 표현을 만들도록 도와준다. 

6) 레이어 정규화(Layer Normalization) 및 잔차 연결(Residual Connection)
레이어 정규화는 모델이 안정적으로 학습하도록 도와주고, 잔차연결은 이전 레이어의 출력을 더해줌으로써 정보 손실을 방지한다. 

7) Decoder에서 최종 출력 생성
트랜스포머의 디코더는 생성된 문장을 바탕으로 새로운 단어를 예측한다. 




3. BiGRU와 Transformer의 비교
BiGRU와 Transformer는 모두 시계열 데이터나 순차 데이터(sequential data)를 처리하기 위해 설계된 딥러닝 모델으로 복잡한 입력 간 관계를 
학습하여 예측 정확도를 향상시키는 것을 목적으로 한다. 
순차 데이터란 순서가 변경될 경우 데이터의 특성을 잃어버리는 데이터를 말한다. 예를 들면 노무 및 인력 투입 기록, 작업 현장의 온도, 습도, 진동 등 
시계열 센서 데이터, 순차적으로 발생하는 하자 보고서 등이 있다. 
순차 데이터를 분석하기 위한 모델은 과거의 데이터를 기억하는 기능이 필요하다. 과거 정보를 기억하기 위해 사용하는 
대표적인 방법은 이전에 사용한 데이터를 재사용하는 방식으로, 순환 신경망 구조(RNN)가 있다. 
순환 신경망 구조에서 발전한 것이 LSTM이고, LSTM의 복잡성을 줄이며 개선된 모델이 GRU이다. 

BiGRU와 Transformer는 모두 다층 구조를 통해 입력의 고차원적 표현을 학습하며 과거의 정보를 반영하여 현재 혹은 미래의 값을 예측한다. 
하지만, 두 모델은 구조적, 처리 방식, 성능 확장적 측면에서 차이점이 있다. 
BiGRU는 RNN계열의 모델로, 순환 구조를 통해 시간 흐름에 따른 입력을 순차적으로 처리하며, 양방향 구조를 채택함으로써 
과거와 미래의 정보를 동시에 반영할 수 있다는 이점을 갖는다. 하지만, 이러한 순환구조는 병렬 처리가 어렵고, 긴 시퀀스를
다룰 때 연산 효율이 떨어지고, 장기 의존성 문제에 취약하다는 약점이 있다. 또한 시퀀스의 길이가 늘어날수록 오래된 정보가 희석되어 학습
과정에서 그래디언트 소실 문제가 발생할 수 있다. 
반면, Transformer모델은 self-attention메커니즘을 기반으로 해 순차적 순환 없이 병렬처리가 가능하기 때문에 학습속도가 빠르며
시퀀스 길이에 관계없이 장기 의존성을 효과적으로 학습할 수 있다. (장기 의존성을 효과적으로 학습한다 = 데이터 내에서 멀리 떨어진 요소들 사이의 관계를 잘 파악함)
BiGRU와 Transformer는 모두 순차적 데이터 처리에 유용하지만, BiGRU는 구조가 단순하고 상대적으로 작은 데이터셋에 적합하며, 
Transformer는 대규모 데이터와 장기 의존성이 요구되는 문제에서 우수한 성능과 학습 효율성을 제공한다. 
다음은 둘의 특성들을 표로 정리해놓은 것이다. 
[ 표 ]




