1. Transformer 하이퍼파라미터 설정
문제점: head_size, num_heads, ff_dim 등의 하이퍼파라미터가 OMA로 최적화되지만, 검색 범위(Lb, Ub)가 부적절하면 성능 저하가 발생할 수 있습니다.
예상 오류: head_size가 임베딩 차원(64)을 정수로 나누지 못하면 MultiHeadAttention 레이어에서 차원 불일치 오류가 발생할 수 있습니다.
해결 방안: head_size를 64의 약수(예: 2, 4, 8, 16, 32)로 제한하거나, 범위를 조정하세요.
2. Transformer 출력 처리
문제점: 마지막 타임스텝(x[:, -1, :])만 사용되는데, 이는 전체 시퀀스 정보를 활용하지 못할 수 있습니다.
예상 오류: 시퀀스 전체의 패턴이 중요한 경우 예측 성능이 저하될 수 있습니다.
해결 방안: 평균 풀링(tf.reduce_mean(x, axis=1))이나 어텐션 메커니즘을 도입해 보세요.
3. Positional Encoding
문제점: 입력을 64차원으로 투영한 후 Positional Encoding이 적용되는데, 차원이 데이터 특성에 맞지 않으면 학습이 비효율적일 수 있습니다.
예상 오류: 차원이 너무 작거나 크면 위치 정보를 제대로 반영하지 못할 수 있습니다.
해결 방안: 투영 차원을 하이퍼파라미터로 추가하여 최적화하거나, 데이터에 맞는 차원을 선택하세요.
4. 학습 안정성
문제점: batch_size=train_y.shape[0]로 설정되어 전체 데이터를 한 번에 학습합니다. 데이터가 크면 메모리 문제가 발생할 수 있습니다.
예상 오류: 대규모 데이터셋에서 OutOfMemoryError가 발생하거나 학습이 느려질 수 있습니다.
해결 방안: batch_size=32 또는 64와 같은 작은 값으로 조정하세요.
5. OMA 최적화 수렴
문제점: OMA가 최적해를 찾지 못하거나 로컬 최적해에 갇힐 수 있습니다.
예상 오류: 부적절한 하이퍼파라미터가 선택되어 모델 성능이 저하될 수 있습니다.
해결 방안: npop과 maxiter를 늘리거나, 여러 번 실행하여 최적 결과를 선택하세요.
6. 코드 일관성
문제점: tensorflow.keras만 사용되므로 라이브러리 혼용 문제는 없지만, 코드 유지보수성을 위해 명시적 버전 확인이 필요할 수 있습니다.
예상 오류: TensorFlow 버전 간 미세한 차이로 인해 동작이 달라질 수 있습니다.
해결 방안: import tensorflow as tf 후 tf.__version__으로 버전을 확인하세요.
7. 데이터 파일
문제점: "ESTC Denorm.csv" 파일이 없거나 경로가 잘못되면 오류가 발생합니다.
예상 오류: FileNotFoundError가 발생할 수 있습니다.
해결 방안: 파일 경로를 확인하고, 파일이 존재하는지 점검하세요.
8. Transformer 레이어 수
문제점: Transformer 블록이 2개로 고정되어 있어 복잡한 시퀀스 패턴을 학습하기에 부족할 수 있습니다.
예상 오류: 모델이 복잡한 의존성을 학습하지 못할 수 있습니다.
해결 방안: 레이어 수를 하이퍼파라미터로 추가하거나, 데이터 복잡성에 따라 조정하세요.
9. 출력 가중치 최적화
문제점: OMA로 가중치를 최적화하는 과정에서 수렴하지 않으면 성능 개선이 없을 수 있습니다.
예상 오류: 최적화된 가중치가 예측을 개선하지 못할 수 있습니다.
해결 방안: maxiter1과 npop1을 조정하거나, 최적화 전후 성능을 비교하세요.
10. 평가 지표
문제점: 정규화된 값(test_y, y_pred_OMA)과 역정규화된 값(test_y_act, y_pred_OMA_act)이 혼용되어 계산됩니다.
예상 오류: MAPE와 같은 지표가 부정확하게 해석될 수 있습니다.
해결 방안: 모든 지표를 역정규화된 값(*_act)에 대해 계산하여 일관성을 유지하세요.
